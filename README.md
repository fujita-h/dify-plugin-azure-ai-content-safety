![Icon](./_assets/03390-icon-service-Content-Safety.svg)

# Azure AI Content Safety

[![GitHub Repo](https://img.shields.io/badge/GitHub_Repo-fujita--h/dify--plugin--azure--ai--content--safety-blue?logo=github)](https://github.com/fujita-h/dify-plugin-azure-ai-content-safety)  
![GitHub Release](https://img.shields.io/github/v/release/fujita-h/dify-plugin-azure-ai-content-safety)
![GitHub License](https://img.shields.io/github/license/fujita-h/dify-plugin-azure-ai-content-safety)


This Plugion provides a set of tools to enhance the safety of generative AI applications with advanced guardrails for responsible AI.
Azure AI Content Safety is an AI service that detects harmful user-generated and AI-generated content in applications and services.

## Tools provided by this plugin

Enhance the safety of generative AI applications with advanced guardrails for responsible AI

### Text Moderation

Scans text for sexual content, violence, hate, and self harm with multi-severity levels.

https://learn.microsoft.com/ja-jp/azure/ai-services/content-safety/concepts/harm-categories?tabs=warning

### Image Moderation

Scans text for sexual content, violence, hate, and self harm with multi-severity levels.

https://learn.microsoft.com/ja-jp/azure/ai-services/content-safety/concepts/harm-categories?tabs=warning

### Prompt Shields

Prompt Shields analyzes LLM input and detects adversarial user input attacks.

https://learn.microsoft.com/azure/ai-services/content-safety/concepts/jailbreak-detection


## Notes

### Supported languages

See the [official documentation](https://learn.microsoft.com/azure/ai-services/content-safety/language-support) for the supported languages.


## Contributing

This plugin is open-source and contributions are welcome. Please visit the GitHub repository to contribute.
